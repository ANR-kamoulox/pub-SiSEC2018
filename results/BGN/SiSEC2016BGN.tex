\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb,bm,multirow}
\begin{document}
\section{Data Description}
For the task `two-channel mixtures of speech and real-world BackGround Noise (BGN)',
we reused the same dataset as in SiSEC 2013 (see~\cite{SiSEC2013}).


\section{Results}
Three algorithms were submitted to the BGN task as shown in Table \ref{tab:BGN}. 
Duong's method~\cite{Duong2015} is based on Nonnegative Matrix Factorization (NMF) with pre-trained speech and noise spectral dictionaries.
Liu's method performs TDOA clustering based on GCC-PHAT.
Wood's method~\cite{Wood2016} first applies NMF to the magnitude spectrograms of the mixture signals with channels concatenated in time. Each dictionary atom  is then attributed to either the speech or the noise according to its spatial origin.




\begin{table*}\footnotesize
\centering
\caption{\footnotesize Results for `Two-channel mixtures of speech and real-world background noise'. 
}
\label{tab:BGN}
(a) Single-channel source estimation
\begin{tabular}{|c|c|ccc|cccccc|}
\hline
\multirow{2}{*}{systems}& \multirow{2}{*}{criteria} & \multicolumn{3}{c|}{dev} & \multicolumn{6}{c|}{test}\\
&&Ca1&Sq1&Su1&Ca1&Ca2&Sq1&Sq2&Su1&Su2\\\hline
\multirow{3}{*}{Duong~\cite{Duong2015}}&SDR&5.6&9.3&4.1&3.7&4.3&10.1&11.6&5.3&4.2\\
&SIR&14.9&15.4&12.1&13.2&15.0&17.9&18.2&19.3&9.3\\
&SAR&6.3&10.7&5.3&4.8&4.9&11.1&12.7&5.5&6.6\\\hline
\multirow{3}{*}{Liu}&SDR&1.9&$-3.0$&$-10.6$&1.6&2.7&$-4.4$&$1.9$&$-12.6$&$-1.2$\\
&SIR&4.0&$-2.9$&$-9.7$&4.5&7.7&$-4.3$&2.4&$-12.2$&0.1\\
&SAR&7.5&16.4&6.9&6.5&5.5&18.8&16.9&10.3&8.0\\\hline
\end{tabular}

\ \\
(b) Multichannel source image estimation (target source)
\begin{tabular}{|c|c|ccc|cccccc|}
\hline
\multirow{2}{*}{systems}& \multirow{2}{*}{criteria} & \multicolumn{3}{c|}{dev} & \multicolumn{6}{c|}{test}\\
&&Ca1&Sq1&Su1&Ca1&Ca2&Sq1&Sq2&Su1&Su2\\\hline
\multirow{8}{*}{Duong~\cite{Duong2015}}
&SDR&9.4&6.9&4.7&9.6&11.0&9.3&10.2&9.8&7.0\\
&ISR&23.1&18.0&17.5&23.4&22.6&15.1&18.7&18.5&19.7\\
&SIR&10.5&9.8&5.4&10.7&12.3&15.6&13.7&12.1&7.4\\
&SAR&16.9&10.3&11.7&17.6&18.3&11.6&13.5&14.2&19.0\\
&OPS&14.3&24.1&11.3&10.1&11.5&25.3&16.4&26.0&11.8\\
&TPS&71.8&65.9&72.4&56.2&58.3&49.2&51.9&73.1&45.3\\
&IPS&11.3&18.2&5.1&17.3&17.3&49.9&47.0&18.0&29.8\\
&APS&78.0&66.8&75.1&82.6&81.9&56.1&78.8&57.8&76.0\\
\hline
\multirow{8}{*}{Liu}
&SDR&$-1.0$&$-8.5$&$-12.8$&$-1.9$&0.1&$-11.0$&$-5.6$&-16.7&$-5.6$\\
&ISR&4.1&1.9&3.8&2.1&2.4&0.6&0.3&2.1&1.4\\
&SIR&4.9&$-2.9$&$-8.0$&5.7&9.1&$-4.4$&2.2&$-11.9$&1.1\\
&SAR&19.7&15.1&7.6&19.3&20.7&17.6&15.9&11.0&13.9\\
&OPS&9.5&14.2&21.1&10.6&8.9&14.2&17.2&31.3&12.6\\
&TPS&42.3&38.8&49.5&45.0&43.2&48.3&56.1&62.5&51.0\\
&IPS&16.8&18.9&15.7&37.0&23.2&47.6&62.5&35.1&50.3\\
&APS&77.1&70.2&60.1&78.6&79.3&76.0&78.6&50.3&80.1\\
\hline
\multirow{8}{*}{Wood~\cite{Wood2016}}
&SDR&3.0&1.9&0.2&2.9&3.1&$-0.7$&2.5&$-2.6$&2.7\\
&ISR&3.7&7.5&2.5&3.7&3.7&12.7&16.0&3.0&5.5\\
&SIR&9.4&2.4&$-2.6$&9.0&12.4&$-0.5$&3.3&$-6.4$&3.8\\
&SAR&5.0&4.0&1.3&5.3&5.2&6.3&8.3&0.3&4.5\\
&OPS&33.7&38.6&25.9&36.6&35.4&45.1&57.7&26.0&44.1\\
&TPS&40.5&57.6&24.4&45.4&42.8&60.2&64.6&20.6&57.2\\
&IPS&60.7&60.5&47.6&66.1&64.5&69.2&74.6&55.4&67.6\\
&APS&39.0&43.3&31.7&41.0&39.5&47.9&61.4&28.0&48.9\\
\hline
\end{tabular}
\end{table*}

\begin{thebibliography}{9}


\bibitem{SiSEC2013}
N.~Ono, Z.~Koldovsky, S.~Miyabe, and N.~Ito, 
\newblock ``The 2013 signal separation evaluation campaign,'' 
\newblock in {\em Proc. MLSP}, Sept. 2013.

\bibitem{Duong2015}
H.-T.T.~Duong, Q.-C.~Nguyen, C.-P.~Nguyen, T.-H.~Tran, and N.Q.K.~Duong,
\newblock ``Speech enhancement based on nonnegative matrix factorization with mixed group sparsity constraint,''
\newblock in {\em Proc. ACM  International Symposium on Information and Communication Technology}, 2015, pp. 247--251.


\bibitem{Wood2016}
S.~Wood and J.~Rouat, 
\newblock ``Blind speech separation with GCC-NMF,'' 
\newblock in {\em Proc. Interspeech}, 2016.
\end{thebibliography}


\end{document}

